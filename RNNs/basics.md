
# **Neural Language Models: A Friendly Guide with Math Examples**

## **1. Introduction: How Computers Predict Words**

Imagine teaching a computer to guess the next word in a sentence, like a super-smart autocomplete. For example, given the phrase:  
**"the cat sat on the ____"**  
A good guess might be **"mat"**‚Äîbut how does the computer figure that out?  

This is what **Neural Language Models (NLMs)** do. They learn patterns from text and predict the most likely next word. Let‚Äôs break it down step by step, first in simple terms, then with the math behind it.

---

## **2. Setting Up the Word Universe (Vocabulary)**
First, we need a small vocabulary of words the model knows:  
**Vocabulary (V):** `["the", "cat", "sat", "on", "mat", "."]`  
(Total words: **6**)  

**Example Sentence:** `"the cat sat on the"`  

### **Why This Matters**  
The model‚Äôs job is to predict the next word after seeing `"the cat sat on the"`. Ideally, it should guess **"mat"** (or maybe **"."** if the sentence ends).  

---

## **3. Turning Words into Numbers (One-Hot Encoding)**
Computers don‚Äôt understand words‚Äîthey need numbers. So, we represent each word as a unique **one-hot vector**:  

| Word | One-Hot Encoding |
|------|------------------|
| "the" | `[1, 0, 0, 0, 0, 0]` |
| "cat" | `[0, 1, 0, 0, 0, 0]` |
| "sat" | `[0, 0, 1, 0, 0, 0]` |
| "on"  | `[0, 0, 0, 1, 0, 0]` |
| "mat" | `[0, 0, 0, 0, 1, 0]` |
| "."   | `[0, 0, 0, 0, 0, 1]` |

### **Why One-Hot Encoding?**  
- Each word gets a unique "ID" the computer can process.  
- Helps the model distinguish between words mathematically.  

---

## **4. The Neural Network‚Äôs Brain (Parameters)**
The model has a **hidden state** (like short-term memory) and **weights** (rules for processing words).  

### **Key Components:**
1. **Hidden Dimension (`d‚Çï`)** = 3  
   - Think of this as the model having **3 "thought slots"** to remember context.  
2. **Weight Matrices (Learned from Data):**  
   - **`W‚Çì‚Çï`**: Converts input words into hidden states.  
   - **`W‚Çï‚Çï`**: Updates the hidden state based on previous memory.  
   - **`W‚Çï·µß`**: Converts hidden states into word predictions.  
3. **Biases (`b‚Çï`, `b·µß`)**  
   - Adjust predictions to fit patterns better.  

---

## **5. Processing Words Step-by-Step**
The model reads words one by one, updating its hidden state each time.  

### **Initial State (`h‚ÇÄ`):**  
`[0, 0, 0]` (No memory yet.)  

### **For Each Word:**
1. **Combine current word (`x‚Çú`) + previous hidden state (`h‚Çú‚Çã‚ÇÅ`).**  
2. **Apply `tanh` (squishes values between -1 and 1).**  
3. **Update hidden state (`h‚Çú`).**  

#### **Example Calculation (Simplified):**
Suppose after processing `"the cat sat on the"`, the final hidden state is:  
`h‚ÇÑ = [0.2, -0.1, 0.4]`  

---

## **6. Predicting the Next Word (Softmax)**
The model converts the hidden state into word probabilities using **softmax**:  

### **Final Output (`≈∑`):**  
`[0.02, 0.08, 0.1, 0.15, 0.6, 0.05]`  

This means:  
- **P("the")** = 2%  
- **P("cat")** = 8%  
- **P("sat")** = 10%  
- **P("on")** = 15%  
- **P("mat")** = 60% üéâ **(Most likely!)**  
- **P(".")** = 5%  

### **Why "mat"?**  
- The model learned that `"on the mat"` is a common phrase.  
- **0.6 (60%)** is the highest probability.  

---

## **7. The Math Behind It (With Examples)**
Now, let‚Äôs formalize the steps with equations.

### **A. Hidden State Update**
At each step `t`, the hidden state `h‚Çú` is computed as:  

\[
h‚Çú = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)
\]

**Where:**  
- `W‚Çï‚Çï` = Hidden-to-hidden weights (shape `3√ó3`)  
- `W‚Çì‚Çï` = Input-to-hidden weights (shape `3√ó6`)  
- `x‚Çú` = One-hot input word (shape `6√ó1`)  
- `b‚Çï` = Hidden bias (shape `3√ó1`)  

**Example:**  
If `h‚ÇÉ = [0.1, -0.2, 0.3]` and `x‚ÇÑ = "the" = [1,0,0,0,0,0]`, then:  
\[
h‚ÇÑ = \tanh(W_{hh} h‚ÇÉ + W_{xh} x‚ÇÑ + b_h)
\]

---

### **B. Output Prediction (Softmax)**
The predicted probabilities are:  

\[
≈∑ = \text{softmax}(W_{hy} h_t + b_y)
\]

**Where:**  
- `W‚Çï·µß` = Hidden-to-output weights (shape `6√ó3`)  
- `b·µß` = Output bias (shape `6√ó1`)  

**Softmax Example:**  
If `W‚Çï·µß h‚ÇÑ + b·µß = [1.0, 0.5, -0.2, 0.3, 2.0, -1.0]`, then:  

\[
\text{softmax}([1.0, 0.5, -0.2, 0.3, 2.0, -1.0]) ‚âà [0.02, 0.08, 0.1, 0.15, 0.6, 0.05]
\]

‚Äî

